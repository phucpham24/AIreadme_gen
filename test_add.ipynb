{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from config import WHITE, GREEN, RESET_COLOR, model_name\n",
    "\n",
    "# Define your OpenAI API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Function to read ctags file and extract function names and locations\n",
    "import json\n",
    "\n",
    "def extract_functions_from_json_ctags(ctags_json_file):\n",
    "    functions = []\n",
    "\n",
    "    with open(ctags_json_file, 'r') as file:\n",
    "        ctags_data = json.load(file)\n",
    "\n",
    "    for entry in ctags_data:\n",
    "        if entry.get('kind') == 'function':\n",
    "            function_name = entry.get('name')\n",
    "            file_path = entry.get('path')\n",
    "            line_number = entry.get('line')\n",
    "            functions.append((function_name, file_path, line_number))\n",
    "\n",
    "    return functions\n",
    "\n",
    "# Function to read call hierarchy JSON file\n",
    "def read_call_hierarchy_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Process the JSON data as needed to extract relevant information\n",
    "    return data  # Return the loaded JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text using OpenAI API with text splitter\n",
    "def generate_text_with_splitter(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100  # Adjust the max_tokens as needed\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "      \n",
    "# Function to add a docstring before a function\n",
    "def add_docstring_to_function(file_path, function_name, docstring):\n",
    "    with open(file_path, 'r') as source_file:\n",
    "        lines = source_file.readlines()\n",
    "\n",
    "    in_function = False\n",
    "    modified_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if the current line contains the function name\n",
    "        if re.match(r'^\\s*def\\s+' + re.escape(function_name) + r'\\s*\\(.*\\):', line):\n",
    "            in_function = True\n",
    "            modified_lines.append(f'def {function_name}():\\n')\n",
    "            modified_lines.append(f'    \"\"\"{docstring}\"\"\"\\n')  # Add the docstring\n",
    "        elif in_function:\n",
    "            # Copy the rest of the function\n",
    "            modified_lines.append(line)\n",
    "        else:\n",
    "            modified_lines.append(line)\n",
    "\n",
    "    with open(file_path, 'w') as source_file:\n",
    "        source_file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Extra data: line 2 column 1 (char 93)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main function\n",
    "folder_path = \"/home/phucsaiyan/Documents/stage/test_addcomment2\"  # Specify the folder path here\n",
    "ctags_file = os.path.join(folder_path, \"/home/phucsaiyan/Documents/stage/test_addcomment2/tags.json\")\n",
    "json_file = os.path.join(folder_path, \"/home/phucsaiyan/Documents/stage/test_addcomment2/full_call_hierarchy.json\")\n",
    "\n",
    "try:\n",
    "    functions = extract_functions_from_json_ctags(ctags_file)\n",
    "    call_hierarchy_data = read_call_hierarchy_json(json_file)\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".py\"):\n",
    "                code_file_path = os.path.join(root, file_name)\n",
    "                for function_name, _ in functions:\n",
    "                    # Use the call hierarchy data to determine which functions need docstrings\n",
    "                    if function_name in call_hierarchy_data:\n",
    "                        prompt = f\"Generate a comment for the function {function_name} based on call hierarchy data:\\n\\n{call_hierarchy_data}\"\n",
    "                        generated_text = generate_text_with_splitter(prompt)\n",
    "                        add_docstring_to_function(code_file_path, function_name, generated_text)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!_TAG_EXTRA_DESCRIPTION', '/Include tags for non-named objects like lambda/'), ('!_TAG_EXTRA_DESCRIPTION', '/Include tags of file scope/'), ('!_TAG_EXTRA_DESCRIPTION', '/Include pseudo tags/'), ('!_TAG_EXTRA_DESCRIPTION', '/Include tags generated by subparsers/'), ('!_TAG_FIELD_DESCRIPTION', '/the last modified time of the input file (only for F\\\\/file kind tag)/'), ('!_TAG_FIELD_DESCRIPTION', '/File-restricted scoping/'), ('!_TAG_FIELD_DESCRIPTION', '/input file/'), ('!_TAG_FIELD_DESCRIPTION', '/tag name/'), ('!_TAG_FIELD_DESCRIPTION', '/pattern/'), ('!_TAG_FIELD_DESCRIPTION', '/Type and name of a variable or typedef/'), ('!_TAG_FIELD_DESCRIPTION!Python', '/the original name for the tag/'), ('!_TAG_FILE_FORMAT', '/extended format; --format=1 will not append ;\" to lines/'), ('!_TAG_FILE_SORTED', '/0=unsorted, 1=sorted, 2=foldcase/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/level 2 sections/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/level 4 sections/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/chapters/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/hashtags/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/footnotes/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/sections/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/level 3 sections/'), ('!_TAG_KIND_DESCRIPTION!Markdown', '/level 5 sections/'), ('!_TAG_KIND_DESCRIPTION!Python', '/name referring a module defined in other file/'), ('!_TAG_KIND_DESCRIPTION!Python', '/name referring a class\\\\/variable\\\\/function\\\\/module defined in other module/'), ('!_TAG_KIND_DESCRIPTION!Python', '/classes/'), ('!_TAG_KIND_DESCRIPTION!Python', '/functions/'), ('!_TAG_KIND_DESCRIPTION!Python', '/modules/'), ('!_TAG_KIND_DESCRIPTION!Python', '/class members/'), ('!_TAG_KIND_DESCRIPTION!Python', '/variables/'), ('!_TAG_OUTPUT_EXCMD', '/number, pattern, mixed, or combineV2/'), ('!_TAG_OUTPUT_FILESEP', '/slash or backslash/'), ('!_TAG_OUTPUT_MODE', '/u-ctags or e-ctags/'), ('!_TAG_OUTPUT_VERSION', '/current.age/'), ('!_TAG_PARSER_VERSION!Markdown', '/current.age/'), ('!_TAG_PARSER_VERSION!Python', '/current.age/'), ('!_TAG_PATTERN_LENGTH_LIMIT', '/0 for no limit/'), ('!_TAG_PROC_CWD', '//'), ('!_TAG_PROGRAM_AUTHOR', '//'), ('!_TAG_PROGRAM_NAME', '/Derived from Exuberant Ctags/'), ('!_TAG_PROGRAM_URL', '/official site/'), ('!_TAG_PROGRAM_VERSION', '/d104ee5b/'), ('!_TAG_ROLE_DESCRIPTION!Python!module', '/imported modules/'), ('!_TAG_ROLE_DESCRIPTION!Python!module', '/module imported in alternative name/'), ('!_TAG_ROLE_DESCRIPTION!Python!module', '/namespace from where classes\\\\/variables\\\\/functions are imported/'), ('!_TAG_ROLE_DESCRIPTION!Python!unknown', '/imported from the other module/'), ('!_TAG_ROLE_DESCRIPTION!Python!unknown', '/classes\\\\/variables\\\\/functions\\\\/modules imported in alternative name/'), ('AI GitHub Reader with ChatGPT-4', '/^# AI GitHub Reader with ChatGPT-4$/;\"'), ('GREEN', '/^GREEN = \"\\\\\\\\033[32m\"$/;\"'), ('How it Works', '/^## How it Works$/;\"'), ('Key Features', '/^## Key Features$/;\"'), ('OPENAI_API_KEY', '/^OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")$/;\"'), ('Prerequisites', '/^## Prerequisites$/;\"'), ('QuestionContext', '/^class QuestionContext:$/;\"'), ('RESET_COLOR', '/^RESET_COLOR = \"\\\\\\\\033[0m\"$/;\"'), ('Usage', '/^## Usage$/;\"'), ('WHITE', '/^WHITE = \"\\\\\\\\033[37m\"$/;\"'), ('__init__', '/^    def __init__(self, index, documents, llm_chain, model_name, repo_name, github_url, conversat/;\"'), ('ask_question', '/^def ask_question(question, context: QuestionContext):$/;\"'), ('clean_and_tokenize', '/^def clean_and_tokenize(text):$/;\"'), ('clone_github_repo', '/^def clone_github_repo():$/;\"'), ('config', '/^    config = Config(max_depth=10)  # Set the maximum depth for the call graph$/;\"'), ('format_documents', '/^def format_documents(documents):$/;\"'), ('format_user_question', '/^def format_user_question(question):$/;\"'), ('graphviz', '/^    graphviz = GraphvizOutput(output_file=\\'call_graph.png\\')  # Output file name$/;\"'), ('load_and_index_files', '/^def load_and_index_files(repo_path):$/;\"'), ('main', '/^def main():$/;\"'), ('model_name', '/^model_name = \"gpt-4-16k-0314\"/;\"'), ('search_documents', '/^def search_documents(query, index, documents, n_results=5):$/;\"')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'add_docstring_to_function': {'name': 'add_docstring_to_function',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'open',\n",
       "    'type': 'call',\n",
       "    'line_number': 33,\n",
       "    'column_offset': 9,\n",
       "    'children': []},\n",
       "   {'name': 'open',\n",
       "    'type': 'call',\n",
       "    'line_number': 51,\n",
       "    'column_offset': 9,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " '__init__': {'name': '__init__',\n",
       "  'type': 'function',\n",
       "  'calls': [],\n",
       "  'children': []},\n",
       " 'ask_question': {'name': 'ask_question',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'search_documents',\n",
       "    'type': 'call',\n",
       "    'line_number': 18,\n",
       "    'column_offset': 20,\n",
       "    'children': []},\n",
       "   {'name': 'format_documents',\n",
       "    'type': 'call',\n",
       "    'line_number': 20,\n",
       "    'column_offset': 25,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'extract_functions_from_ctags': {'name': 'extract_functions_from_ctags',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'open',\n",
       "    'type': 'call',\n",
       "    'line_number': 13,\n",
       "    'column_offset': 9,\n",
       "    'children': []},\n",
       "   {'name': 'len',\n",
       "    'type': 'call',\n",
       "    'line_number': 16,\n",
       "    'column_offset': 15,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'generate_docstring': {'name': 'generate_docstring',\n",
       "  'type': 'function',\n",
       "  'calls': [],\n",
       "  'children': []},\n",
       " 'main': {'name': 'main',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'input',\n",
       "    'type': 'call',\n",
       "    'line_number': 21,\n",
       "    'column_offset': 17,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 23,\n",
       "    'column_offset': 4,\n",
       "    'children': []},\n",
       "   {'name': 'clone_github_repo',\n",
       "    'type': 'call',\n",
       "    'line_number': 25,\n",
       "    'column_offset': 11,\n",
       "    'children': []},\n",
       "   {'name': 'load_and_index_files',\n",
       "    'type': 'call',\n",
       "    'line_number': 26,\n",
       "    'column_offset': 60,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 28,\n",
       "    'column_offset': 16,\n",
       "    'children': []},\n",
       "   {'name': 'exit',\n",
       "    'type': 'call',\n",
       "    'line_number': 29,\n",
       "    'column_offset': 16,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 31,\n",
       "    'column_offset': 12,\n",
       "    'children': []},\n",
       "   {'name': 'OpenAI',\n",
       "    'type': 'call',\n",
       "    'line_number': 32,\n",
       "    'column_offset': 18,\n",
       "    'children': []},\n",
       "   {'name': 'PromptTemplate',\n",
       "    'type': 'call',\n",
       "    'line_number': 49,\n",
       "    'column_offset': 21,\n",
       "    'children': []},\n",
       "   {'name': 'LLMChain',\n",
       "    'type': 'call',\n",
       "    'line_number': 54,\n",
       "    'column_offset': 24,\n",
       "    'children': []},\n",
       "   {'name': 'QuestionContext',\n",
       "    'type': 'call',\n",
       "    'line_number': 57,\n",
       "    'column_offset': 31,\n",
       "    'children': []},\n",
       "   {'name': 'input',\n",
       "    'type': 'call',\n",
       "    'line_number': 60,\n",
       "    'column_offset': 36,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 63,\n",
       "    'column_offset': 20,\n",
       "    'children': []},\n",
       "   {'name': 'format_user_question',\n",
       "    'type': 'call',\n",
       "    'line_number': 64,\n",
       "    'column_offset': 36,\n",
       "    'children': []},\n",
       "   {'name': 'ask_question',\n",
       "    'type': 'call',\n",
       "    'line_number': 66,\n",
       "    'column_offset': 29,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 67,\n",
       "    'column_offset': 20,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 70,\n",
       "    'column_offset': 20,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 74,\n",
       "    'column_offset': 12,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'clean_and_tokenize': {'name': 'clean_and_tokenize',\n",
       "  'type': 'function',\n",
       "  'calls': [],\n",
       "  'children': []},\n",
       " 'format_documents': {'name': 'format_documents',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'enumerate',\n",
       "    'type': 'call',\n",
       "    'line_number': 20,\n",
       "    'column_offset': 118,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'format_user_question': {'name': 'format_user_question',\n",
       "  'type': 'function',\n",
       "  'calls': [],\n",
       "  'children': []},\n",
       " 'build_hierarchy': {'name': 'build_hierarchy',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'isinstance',\n",
       "    'type': 'call',\n",
       "    'line_number': 6,\n",
       "    'column_offset': 7,\n",
       "    'children': []},\n",
       "   {'name': 'build_hierarchy',\n",
       "    'type': 'call',\n",
       "    'line_number': 11,\n",
       "    'column_offset': 25,\n",
       "    'children': []},\n",
       "   {'name': 'isinstance',\n",
       "    'type': 'call',\n",
       "    'line_number': 11,\n",
       "    'column_offset': 74,\n",
       "    'children': []},\n",
       "   {'name': 'isinstance',\n",
       "    'type': 'call',\n",
       "    'line_number': 13,\n",
       "    'column_offset': 9,\n",
       "    'children': []},\n",
       "   {'name': 'hasattr',\n",
       "    'type': 'call',\n",
       "    'line_number': 13,\n",
       "    'column_offset': 40,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'visit_FunctionDef': {'name': 'visit_FunctionDef',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'build_hierarchy',\n",
       "    'type': 'call',\n",
       "    'line_number': 30,\n",
       "    'column_offset': 53,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'visit_Call': {'name': 'visit_Call',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'hasattr',\n",
       "    'type': 'call',\n",
       "    'line_number': 34,\n",
       "    'column_offset': 11,\n",
       "    'children': []},\n",
       "   {'name': 'build_hierarchy',\n",
       "    'type': 'call',\n",
       "    'line_number': 35,\n",
       "    'column_offset': 24,\n",
       "    'children': []},\n",
       "   {'name': 'open',\n",
       "    'type': 'call',\n",
       "    'line_number': 53,\n",
       "    'column_offset': 17,\n",
       "    'children': []},\n",
       "   {'name': 'CallHierarchyVisitor',\n",
       "    'type': 'call',\n",
       "    'line_number': 59,\n",
       "    'column_offset': 22,\n",
       "    'children': []},\n",
       "   {'name': 'open',\n",
       "    'type': 'call',\n",
       "    'line_number': 69,\n",
       "    'column_offset': 5,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'clone_github_repo': {'name': 'clone_github_repo',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 17,\n",
       "    'column_offset': 8,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'load_and_index_files': {'name': 'load_and_index_files',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'NotebookLoader',\n",
       "    'type': 'call',\n",
       "    'line_number': 31,\n",
       "    'column_offset': 25,\n",
       "    'children': []},\n",
       "   {'name': 'str',\n",
       "    'type': 'call',\n",
       "    'line_number': 31,\n",
       "    'column_offset': 40,\n",
       "    'children': []},\n",
       "   {'name': 'DirectoryLoader',\n",
       "    'type': 'call',\n",
       "    'line_number': 33,\n",
       "    'column_offset': 25,\n",
       "    'children': []},\n",
       "   {'name': 'callable',\n",
       "    'type': 'call',\n",
       "    'line_number': 35,\n",
       "    'column_offset': 48,\n",
       "    'children': []},\n",
       "   {'name': 'len',\n",
       "    'type': 'call',\n",
       "    'line_number': 37,\n",
       "    'column_offset': 40,\n",
       "    'children': []},\n",
       "   {'name': 'str',\n",
       "    'type': 'call',\n",
       "    'line_number': 41,\n",
       "    'column_offset': 30,\n",
       "    'children': []},\n",
       "   {'name': 'print',\n",
       "    'type': 'call',\n",
       "    'line_number': 47,\n",
       "    'column_offset': 12,\n",
       "    'children': []},\n",
       "   {'name': 'RecursiveCharacterTextSplitter',\n",
       "    'type': 'call',\n",
       "    'line_number': 50,\n",
       "    'column_offset': 20,\n",
       "    'children': []},\n",
       "   {'name': 'clean_and_tokenize',\n",
       "    'type': 'call',\n",
       "    'line_number': 63,\n",
       "    'column_offset': 31,\n",
       "    'children': []},\n",
       "   {'name': 'BM25Okapi',\n",
       "    'type': 'call',\n",
       "    'line_number': 64,\n",
       "    'column_offset': 16,\n",
       "    'children': []}],\n",
       "  'children': []},\n",
       " 'search_documents': {'name': 'search_documents',\n",
       "  'type': 'function',\n",
       "  'calls': [{'name': 'clean_and_tokenize',\n",
       "    'type': 'call',\n",
       "    'line_number': 68,\n",
       "    'column_offset': 19,\n",
       "    'children': []},\n",
       "   {'name': 'TfidfVectorizer',\n",
       "    'type': 'call',\n",
       "    'line_number': 75,\n",
       "    'column_offset': 23,\n",
       "    'children': []},\n",
       "   {'name': 'cosine_similarity',\n",
       "    'type': 'call',\n",
       "    'line_number': 82,\n",
       "    'column_offset': 24,\n",
       "    'children': []},\n",
       "   {'name': 'list',\n",
       "    'type': 'call',\n",
       "    'line_number': 92,\n",
       "    'column_offset': 34,\n",
       "    'children': []},\n",
       "   {'name': 'set',\n",
       "    'type': 'call',\n",
       "    'line_number': 92,\n",
       "    'column_offset': 39,\n",
       "    'children': []}],\n",
       "  'children': []}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(functions)\n",
    "call_hierarchy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = extract_functions_from_ctags('/home/phucsaiyan/Documents/stage/test_addcomment2/tags.json')\n",
    "for function in functions:\n",
    "    print(f\"Function Name: {function[0]}, File: {function[1]}, Line Number: {function[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading files with pattern '**/*.ipynb': [Errno 21] Is a directory: '/home/phucsaiyan/Documents/stage/test_addcomment2'\n",
      "Problematic folder path: /home/phucsaiyan/Documents/stage/test_addcomment2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.document_loaders import DirectoryLoader, NotebookLoader,TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from utils import clean_and_tokenize\n",
    "import json\n",
    "import glob \n",
    "from tabulate import tabulate\n",
    "def load_folder_documents(folder_path):\n",
    "    extensions = ['txt', 'md', 'markdown', 'rst', 'py', 'js', 'java', 'c', 'cpp', 'cs', 'go', 'rb', 'php', 'scala', 'html', 'htm', 'xml', 'json', 'yaml', 'yml', 'ini', 'toml', 'cfg', 'conf', 'sh', 'bash', 'css', 'scss', 'sql', 'gitignore', 'dockerignore', 'editorconfig', 'ipynb']\n",
    "    documents_dict = {}\n",
    "\n",
    "    for ext in extensions:\n",
    "        glob_pattern = f'**/*.{ext}'\n",
    "        try:\n",
    "            loader = None\n",
    "            if ext == 'ipynb':\n",
    "                loader = NotebookLoader(folder_path, include_outputs=True, max_output_length=20, remove_newline=True)\n",
    "            else:\n",
    "                loader = DirectoryLoader(folder_path, glob=glob_pattern, loader_cls=TextLoader)\n",
    "\n",
    "            loaded_documents = loader.load() if callable(loader.load) else []\n",
    "            \n",
    "            if loaded_documents:\n",
    "                for doc in loaded_documents:\n",
    "                    file_path = doc.metadata['source']\n",
    "                    relative_path = os.path.relpath(file_path, folder_path)\n",
    "                    file_id = str(uuid.uuid4())\n",
    "                    doc.metadata['source'] = relative_path\n",
    "                    doc.metadata['file_id'] = file_id\n",
    "\n",
    "                    documents_dict[file_id] = doc\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files with pattern '{glob_pattern}': {e}\")\n",
    "            print(f\"Problematic folder path: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "    return list(documents_dict.values())\n",
    "\n",
    "def load_and_index_files(repo_path):\n",
    "    main_documents = load_folder_documents(repo_path)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "\n",
    "    split_documents = []\n",
    "    for original_doc in main_documents:\n",
    "        split_docs = text_splitter.split_documents([original_doc])\n",
    "        for split_doc in split_docs:\n",
    "            split_doc.metadata['file_id'] = original_doc.metadata['file_id']\n",
    "            split_doc.metadata['source'] = original_doc.metadata['source']\n",
    "\n",
    "        split_documents.extend(split_docs)\n",
    "\n",
    "    index = None\n",
    "    if split_documents:\n",
    "        tokenized_documents = [clean_and_tokenize(doc.page_content) for doc in split_documents]\n",
    "        index = BM25Okapi(tokenized_documents)\n",
    "    \n",
    "    file_sources = [doc.metadata['source'] for doc in split_documents]\n",
    "    # Create a list of lists to represent your data\n",
    "    data = []\n",
    "    for doc in split_documents:\n",
    "        data.append([doc.metadata['file_id'], doc.metadata['source'], doc.page_content])\n",
    "\n",
    "    return index, split_documents, file_sources, [doc.metadata['source'] for doc in split_documents]\n",
    "\n",
    "\n",
    "ctag = '/home/phucsaiyan/Documents/stage/test_addcomment2'\n",
    "\n",
    "a,b,c,d = load_and_index_files(ctag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading files with pattern '**/*.ipynb': [Errno 21] Is a directory: '/home/phucsaiyan/Documents/stage/test_addcomment2'\n",
      "Problematic folder path: /home/phucsaiyan/Documents/stage/test_addcomment2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.document_loaders import DirectoryLoader, NotebookLoader,TextLoader\n",
    "import json\n",
    "\n",
    "folder_path = '/home/phucsaiyan/Documents/stage/test_addcomment2'\n",
    "# Assuming you have already loaded the documents using the provided code\n",
    "loaded_documents = load_folder_documents(folder_path)\n",
    "def load_folder_documents(folder_path):\n",
    "    extensions = ['txt', 'md', 'markdown', 'rst', 'py', 'js', 'java', 'c', 'cpp', 'cs', 'go', 'rb', 'php', 'scala', 'html', 'htm', 'xml', 'json', 'yaml', 'yml', 'ini', 'toml', 'cfg', 'conf', 'sh', 'bash', 'css', 'scss', 'sql', 'gitignore', 'dockerignore', 'editorconfig', 'ipynb']\n",
    "    documents_dict = {}\n",
    "\n",
    "    for ext in extensions:\n",
    "        glob_pattern = f'**/*.{ext}'\n",
    "        try:\n",
    "            loader = None\n",
    "            if ext == 'ipynb':\n",
    "                loader = NotebookLoader(folder_path, include_outputs=True, max_output_length=20, remove_newline=True)\n",
    "            else:\n",
    "                loader = DirectoryLoader(folder_path, glob=glob_pattern, loader_cls=TextLoader)\n",
    "\n",
    "            loaded_documents = loader.load() if callable(loader.load) else []\n",
    "            \n",
    "            if loaded_documents:\n",
    "                for doc in loaded_documents:\n",
    "                    file_path = doc.metadata['source']\n",
    "                    relative_path = os.path.relpath(file_path, folder_path)\n",
    "                    file_id = str(uuid.uuid4())\n",
    "                    doc.metadata['source'] = relative_path\n",
    "                    doc.metadata['file_id'] = file_id\n",
    "\n",
    "                    documents_dict[file_id] = doc\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files with pattern '{glob_pattern}': {e}\")\n",
    "            print(f\"Problematic folder path: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "    return list(documents_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from main import main\\n\\nmain()', metadata={'source': 'app.py', 'file_id': 'a96b5ed6-5d84-4781-8260-1fd61efddbd8'}),\n",
       " Document(page_content='# file_processing.py\\nimport os\\nimport uuid\\nimport subprocess\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom rank_bm25 import BM25Okapi\\nfrom langchain.document_loaders import DirectoryLoader, NotebookLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom utils import clean_and_tokenize\\n\\ndef clone_github_repo():\\n    \"\"\"This function appears to clone a github repository.\"\"\"\\n    \"\"\"The clone_github_repo function is used to clone a GitHub repository. This function is called by the main function.\"\"\"\\n    \"\"\"This function is responsible for cloning a GitHub repository.\"\"\"\\n    try:\\n        subprocess.run([\\'git\\', \\'clone\\', github_url, local_path], check=True)\\n        return True\\n    except subprocess.CalledProcessError as e:\\n        print(f\"Failed to clone repository: {e}\")\\n        return False\\ndef load_folder_documents(folder_path):\\n    extensions = [\\'txt\\', \\'md\\', \\'markdown\\', \\'rst\\', \\'py\\', \\'js\\', \\'java\\', \\'c\\', \\'cpp\\', \\'cs\\', \\'go\\', \\'rb\\', \\'php\\', \\'scala\\', \\'html\\', \\'htm\\', \\'xml\\', \\'json\\', \\'yaml\\', \\'yml\\', \\'ini\\', \\'toml\\', \\'cfg\\', \\'conf\\', \\'sh\\', \\'bash\\', \\'css\\', \\'scss\\', \\'sql\\', \\'gitignore\\', \\'dockerignore\\', \\'editorconfig\\', \\'ipynb\\']\\n    documents_dict = {}\\n\\n    for ext in extensions:\\n        glob_pattern = f\\'**/*.{ext}\\'\\n        try:\\n            loader = None\\n            if ext == \\'ipynb\\':\\n                loader = NotebookLoader(folder_path, include_outputs=True, max_output_length=20, remove_newline=True)\\n            else:\\n                loader = DirectoryLoader(folder_path, glob=glob_pattern)\\n\\n            loaded_documents = loader.load() if callable(loader.load) else []\\n            \\n            # Filter out directories from the list of loaded documents\\n            loaded_documents = [doc for doc in loaded_documents if not os.path.isdir(doc.metadata[\\'source\\'])]\\n            \\n            if loaded_documents:\\n                for doc in loaded_documents:\\n                    file_path = doc.metadata[\\'source\\']\\n                    relative_path = os.path.relpath(file_path, folder_path)\\n                    file_id = str(uuid.uuid4())\\n                    doc.metadata[\\'source\\'] = relative_path\\n                    doc.metadata[\\'file_id\\'] = file_id\\n\\n                    documents_dict[file_id] = doc\\n        except Exception as e:\\n            print(f\"Error loading files with pattern \\'{glob_pattern}\\': {e}\")\\n            print(f\"Problematic folder path: {folder_path}\")\\n            continue\\n\\n    return list(documents_dict.values())\\n\\ndef load_and_index_files():\\n    \"\"\"This function is responsible for loading and indexing files. It makes calls to the NotebookLoader and DirectoryLoader functions to load files, and the clean_and_tokenize and TfidfVectorizer functions to index the files.\"\"\"\\n    \"\"\"The load_and_index_files function loads and indexes files for the search_documents function.\"\"\"\\n    \"\"\"The load_and_index_files function is responsible for loading and indexing files. It takes in a file path and loads the file using a NotebookLoader or DirectoryLoader. It then cleans and tokenizes the text and indexes it using a BM25Okapi.\"\"\"\\n    main_documents = load_folder_documents(repo_path)\\n\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\\n\\n    split_documents = []\\n    for original_doc in main_documents:\\n        split_docs = text_splitter.split_documents([original_doc])\\n        for split_doc in split_docs:\\n            split_doc.metadata[\\'file_id\\'] = original_doc.metadata[\\'file_id\\']\\n            split_doc.metadata[\\'source\\'] = original_doc.metadata[\\'source\\']\\n\\n        split_documents.extend(split_docs)\\n\\n    index = None\\n    if split_documents:\\n        tokenized_documents = [clean_and_tokenize(doc.page_content) for doc in split_documents]\\n        index = BM25Okapi(tokenized_documents)\\n    \\n    file_sources = [doc.metadata[\\'source\\'] for doc in split_documents]\\n    return index, split_documents, file_sources, [doc.metadata[\\'source\\'] for doc in split_documents]\\n\\ndef search_documents():\\n    \"\"\"This function searches documents for a given question. It firstly pre-processes and tokenizes the question; then it creates a vector representation of the question using a tf-idf vectorizer. Cosine similarity is then used to find the most similar documents to the question. Finally, a list of the most similar documents is returned.\"\"\"\\n    \"\"\"This function serves as the main interface for searching through documents. It accepts a user\\'s question as input, cleans and tokenizes the question, and then searches through a set of documents to find the best match. Additionally, it can also generate a docstring for the best match.\"\"\"\\n    \"\"\"This function searches the documents for a given query. It starts by cleaning and tokenizing the query, then vectorizing it and calculating the cosine similarity between the query and each document. Finally, it returns a list of the indices of the documents most similar to the query.\"\"\"\\n    query_tokens = clean_and_tokenize(query)\\n    bm25_scores = index.get_scores(query_tokens)\\n\\n\\n\\n\\n    # Compute TF-IDF scores\\n    tfidf_vectorizer = TfidfVectorizer(tokenizer=clean_and_tokenize, lowercase=True, stop_words=\\'english\\', use_idf=True, smooth_idf=True, sublinear_tf=True)\\n    tfidf_matrix = tfidf_vectorizer.fit_transform([doc.page_content for doc in documents])\\n    query_tfidf = tfidf_vectorizer.transform([query])\\n\\n\\n\\n    # Compute Cosine Similarity scores\\n    cosine_sim_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\\n\\n\\n\\n    # Combine BM25 and Cosine Similarity scores\\n    combined_scores = bm25_scores * 0.5 + cosine_sim_scores * 0.5\\n\\n\\n\\n    # Get unique top documents\\n    unique_top_document_indices = list(set(combined_scores.argsort()[::-1]))[:n_results]\\n    \\n    return [documents[i] for i in unique_top_document_indices]', metadata={'source': 'file_processing.py', 'file_id': '17c08e2a-ec00-4550-abba-c01017e3d828'}),\n",
       " Document(page_content='[\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"JSON_OUTPUT_VERSION\",\\n        \"path\": \"1.0\",\\n        \"pattern\": \"in development\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_EXTRA_DESCRIPTION\",\\n        \"path\": \"anonymous\",\\n        \"pattern\": \"Include tags for non-named objects like lambda\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_EXTRA_DESCRIPTION\",\\n        \"path\": \"fileScope\",\\n        \"pattern\": \"Include tags of file scope\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_EXTRA_DESCRIPTION\",\\n        \"path\": \"pseudo\",\\n        \"pattern\": \"Include pseudo tags\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_EXTRA_DESCRIPTION\",\\n        \"path\": \"qualified\",\\n        \"pattern\": \"Include an extra class-qualified tag entry for each tag\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_EXTRA_DESCRIPTION\",\\n        \"path\": \"subparser\",\\n        \"pattern\": \"Include tags generated by subparsers\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"nameref\",\\n        \"pattern\": \"the original name for the tag\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"epoch\",\\n        \"pattern\": \"the last modified time of the input file (only for F/file kind tag)\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"file\",\\n        \"pattern\": \"File-restricted scoping\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"input\",\\n        \"pattern\": \"input file\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"language\",\\n        \"pattern\": \"Language of input file containing tag\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"name\",\\n        \"pattern\": \"tag name\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"pattern\",\\n        \"pattern\": \"pattern\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FIELD_DESCRIPTION\",\\n        \"path\": \"typeref\",\\n        \"pattern\": \"Type and name of a variable or typedef\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_FILE_SORTED\",\\n        \"path\": \"1\",\\n        \"pattern\": \"0=unsorted, 1=sorted, 2=foldcase\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"I,namespace\",\\n        \"pattern\": \"name referring a module defined in other file\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"Y,unknown\",\\n        \"pattern\": \"name referring a class/variable/function/module defined in other module\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"c,class\",\\n        \"pattern\": \"classes\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"f,function\",\\n        \"pattern\": \"functions\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"i,module\",\\n        \"pattern\": \"modules\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"m,member\",\\n        \"pattern\": \"class members\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_KIND_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"v,variable\",\\n        \"pattern\": \"variables\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_OUTPUT_EXCMD\",\\n        \"path\": \"mixed\",\\n        \"pattern\": \"number, pattern, mixed, or combineV2\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_OUTPUT_FILESEP\",\\n        \"path\": \"slash\",\\n        \"pattern\": \"slash or backslash\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_OUTPUT_VERSION\",\\n        \"path\": \"0.0\",\\n        \"pattern\": \"current.age\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PARSER_VERSION\",\\n        \"parserName\": \"Python\",\\n        \"path\": \"0.0\",\\n        \"pattern\": \"current.age\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PATTERN_LENGTH_LIMIT\",\\n        \"path\": \"96\",\\n        \"pattern\": \"0 for no limit\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PROC_CWD\",\\n        \"path\": \"/home/phucsaiyan/Documents/stage/test_addcomment2/\",\\n        \"pattern\": \"\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PROGRAM_AUTHOR\",\\n        \"path\": \"Universal Ctags Team\",\\n        \"pattern\": \"\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PROGRAM_NAME\",\\n        \"path\": \"Universal Ctags\",\\n        \"pattern\": \"Derived from Exuberant Ctags\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PROGRAM_URL\",\\n        \"path\": \"https://ctags.io/\",\\n        \"pattern\": \"official site\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_PROGRAM_VERSION\",\\n        \"path\": \"6.0.0\",\\n        \"pattern\": \"d104ee5b\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_ROLE_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"kindName\": \"module\",\\n        \"path\": \"imported\",\\n        \"pattern\": \"imported modules\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_ROLE_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"kindName\": \"module\",\\n        \"path\": \"indirectlyImported\",\\n        \"pattern\": \"module imported in alternative name\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_ROLE_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"kindName\": \"module\",\\n        \"path\": \"namespace\",\\n        \"pattern\": \"namespace from where classes/variables/functions are imported\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_ROLE_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"kindName\": \"unknown\",\\n        \"path\": \"imported\",\\n        \"pattern\": \"imported from the other module\"\\n    },\\n    {\\n        \"_type\": \"ptag\",\\n        \"name\": \"TAG_ROLE_DESCRIPTION\",\\n        \"parserName\": \"Python\",\\n        \"kindName\": \"unknown\",\\n        \"path\": \"indirectlyImported\",\\n        \"pattern\": \"classes/variables/functions/modules imported in alternative name\"\\n    },\\n    {\\n        \"_type\": \"tag\",\\n        \"name\": \"clone_github_repo\",\\n        \"path\": \"/home/phucsaiyan/Documents/stage/test_addcomment2/file_processing.py\",\\n        \"pattern\": \"/^def clone_github_repo():$/\",\\n        \"language\": \"Python\",\\n        \"kind\": \"function\"\\n    },\\n    {\\n        \"_type\": \"tag\",\\n        \"name\": \"load_and_index_files\",\\n        \"path\": \"/home/phucsaiyan/Documents/stage/test_addcomment2/file_processing.py\",\\n        \"pattern\": \"/^def load_and_index_files():$/\",\\n        \"language\": \"Python\",\\n        \"kind\": \"function\"\\n    },\\n    {\\n        \"_type\": \"tag\",\\n        \"name\": \"load_folder_documents\",\\n        \"path\": \"/home/phucsaiyan/Documents/stage/test_addcomment2/file_processing.py\",\\n        \"pattern\": \"/^def load_folder_documents(folder_path):$/\",\\n        \"language\": \"Python\",\\n        \"kind\": \"function\"\\n    },\\n    {\\n        \"_type\": \"tag\",\\n        \"name\": \"search_documents\",\\n        \"path\": \"/home/phucsaiyan/Documents/stage/test_addcomment2/file_processing.py\",\\n        \"pattern\": \"/^def search_documents():$/\",\\n        \"language\": \"Python\",\\n        \"kind\": \"function\"\\n    }\\n]', metadata={'source': 'output.json', 'file_id': '610145a3-4c84-4363-b225-5dfb78db4276'}),\n",
       " Document(page_content='{\\n  \"clone_github_repo\": {\\n    \"name\": \"clone_github_repo\",\\n    \"type\": \"function\",\\n    \"calls\": [\\n      {\\n        \"name\": \"print\",\\n        \"type\": \"call\",\\n        \"line_number\": 17,\\n        \"column_offset\": 8,\\n        \"children\": []\\n      }\\n    ],\\n    \"children\": []\\n  },\\n  \"load_folder_documents\": {\\n    \"name\": \"load_folder_documents\",\\n    \"type\": \"function\",\\n    \"calls\": [\\n      {\\n        \"name\": \"NotebookLoader\",\\n        \"type\": \"call\",\\n        \"line_number\": 28,\\n        \"column_offset\": 25,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"DirectoryLoader\",\\n        \"type\": \"call\",\\n        \"line_number\": 30,\\n        \"column_offset\": 25,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"callable\",\\n        \"type\": \"call\",\\n        \"line_number\": 32,\\n        \"column_offset\": 48,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"str\",\\n        \"type\": \"call\",\\n        \"line_number\": 41,\\n        \"column_offset\": 30,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"print\",\\n        \"type\": \"call\",\\n        \"line_number\": 47,\\n        \"column_offset\": 12,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"print\",\\n        \"type\": \"call\",\\n        \"line_number\": 48,\\n        \"column_offset\": 12,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"list\",\\n        \"type\": \"call\",\\n        \"line_number\": 51,\\n        \"column_offset\": 11,\\n        \"children\": []\\n      }\\n    ],\\n    \"children\": []\\n  },\\n  \"load_and_index_files\": {\\n    \"name\": \"load_and_index_files\",\\n    \"type\": \"function\",\\n    \"calls\": [\\n      {\\n        \"name\": \"load_folder_documents\",\\n        \"type\": \"call\",\\n        \"line_number\": 54,\\n        \"column_offset\": 21,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"RecursiveCharacterTextSplitter\",\\n        \"type\": \"call\",\\n        \"line_number\": 56,\\n        \"column_offset\": 20,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"clean_and_tokenize\",\\n        \"type\": \"call\",\\n        \"line_number\": 69,\\n        \"column_offset\": 31,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"BM25Okapi\",\\n        \"type\": \"call\",\\n        \"line_number\": 70,\\n        \"column_offset\": 16,\\n        \"children\": []\\n      }\\n    ],\\n    \"children\": []\\n  },\\n  \"search_documents\": {\\n    \"name\": \"search_documents\",\\n    \"type\": \"function\",\\n    \"calls\": [\\n      {\\n        \"name\": \"clean_and_tokenize\",\\n        \"type\": \"call\",\\n        \"line_number\": 76,\\n        \"column_offset\": 19,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"TfidfVectorizer\",\\n        \"type\": \"call\",\\n        \"line_number\": 83,\\n        \"column_offset\": 23,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"cosine_similarity\",\\n        \"type\": \"call\",\\n        \"line_number\": 90,\\n        \"column_offset\": 24,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"list\",\\n        \"type\": \"call\",\\n        \"line_number\": 100,\\n        \"column_offset\": 34,\\n        \"children\": []\\n      },\\n      {\\n        \"name\": \"set\",\\n        \"type\": \"call\",\\n        \"line_number\": 100,\\n        \"column_offset\": 39,\\n        \"children\": []\\n      }\\n    ],\\n    \"children\": []\\n  }\\n}', metadata={'source': 'full_call_hierarchy.json', 'file_id': '95f6468e-fdad-4d21-8130-7ef5c942ae4c'})]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def search_function_names(json_data):\n",
    "    function_names = []\n",
    "\n",
    "    def search(json_obj):\n",
    "        if isinstance(json_obj, dict):\n",
    "            for key, value in json_obj.items():\n",
    "                if callable(value) and key not in function_names:\n",
    "                    function_names.append(key)\n",
    "                search(value)\n",
    "        elif isinstance(json_obj, list):\n",
    "            for item in json_obj:\n",
    "                search(item)\n",
    "\n",
    "    search(json_data)\n",
    "    return function_names\n",
    "search_function_names(loaded_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m     20\u001b[0m ctagfile_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/phucsaiyan/Documents/stage/test_addcomment2/output.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m function_names \u001b[39m=\u001b[39m search_function_names_in_ctagfile(ctagfile_path)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(function_names)\n",
      "Cell \u001b[0;32mIn[138], line 11\u001b[0m, in \u001b[0;36msearch_function_names_in_ctagfile\u001b[0;34m(ctagfile_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m function_names \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39m# Assuming the JSON structure, iterate through the keys and values\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m ctag_data\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[1;32m     13\u001b[0m         \u001b[39m# Check if the key represents a function name\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m value \u001b[39mand\u001b[39;00m value[\u001b[39m'\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfunction\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def search_function_names_in_ctagfile(ctagfile_path):\n",
    "    # Read the JSON data from the ctagfile\n",
    "    with open(ctagfile_path, 'r') as file:\n",
    "        ctag_data = json.load(file)\n",
    "\n",
    "    function_names = []\n",
    "\n",
    "    # Assuming the JSON structure, iterate through the keys and values\n",
    "    for key, value in ctag_data.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Check if the key represents a function name\n",
    "            if 'kind' in value and value['kind'] == 'function':\n",
    "                function_names.append(key)\n",
    "\n",
    "    return function_names\n",
    "\n",
    "# Example usage\n",
    "ctagfile_path = '/home/phucsaiyan/Documents/stage/test_addcomment2/output.json'\n",
    "function_names = search_function_names_in_ctagfile(ctagfile_path)\n",
    "print(function_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = '/home/phucsaiyan/Documents/stage/test_addcomment2/tags.jsonl'\n",
    "\n",
    "# Initialize a list to store the JSON objects\n",
    "json_data_list = []\n",
    "\n",
    "# Open and read the JSONL file line by line\n",
    "with open(jsonl_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object and append it to the list\n",
    "        json_data = json.loads(line)\n",
    "        json_data_list.append(json_data)\n",
    "\n",
    "# Now you have a list of JSON objects in json_data_list\n",
    "# You can access and manipulate the data as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open JSONL file for reading\n",
    "with open('/home/phucsaiyan/Documents/stage/test_addcomment2/tags.jsonl', 'r') as jsonl_file:\n",
    "    # Read and convert JSONL data to a list of dictionaries\n",
    "    json_data = [json.loads(line) for line in jsonl_file]\n",
    "\n",
    "# Write the converted data to a standard JSON file\n",
    "with open('output.json', 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path: /home/phucsaiyan/Documents/stage/test_addcomment2/file.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clone_github_repo\n",
      "load_and_index_files\n",
      "load_folder_documents\n",
      "search_documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/home/phucsaiyan/Documents/stage/test_addcomment2/output.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "function_names = []\n",
    "for item in data:\n",
    "    if item['_type'] == 'tag' and item['kind'] == 'function':\n",
    "        function_names.append(item['name'])\n",
    "for function_name in function_names:\n",
    "    print(function_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_names_from_json(json_file_path):\n",
    "    # Open the JSON file and load its data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Initialize a list to store function names\n",
    "    function_names = []\n",
    "\n",
    "    # Iterate through the data and extract function names\n",
    "    for item in data:\n",
    "        if item.get('_type') == 'tag' and item.get('kind') == 'function':\n",
    "            function_names.append(item.get('name'))\n",
    "\n",
    "    return function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_path= '/home/phucsaiyan/Documents/stage/clonechatgpt/ctag.json'\n",
    "a= extract_function_names_from_json(json_file_path)\n",
    "len(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
