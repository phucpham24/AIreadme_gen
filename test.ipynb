{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/phucsaiyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTAGs generated for folder: /home/phucsaiyan/Documents/stage/testadddoc\n",
      "All CTAGs generated in: /home/phucsaiyan/Documents/stage/testadddoc/ctag.jsonl\n",
      "callgraph processed successfully.\n",
      "A new file ctag.json was created.\n",
      "A new file callgraph.json was created.\n",
      "jsonize code is done\n",
      "Error loading files with pattern '**/*.txt': 'utf-8' codec can't decode byte 0xfb in position 0: invalid start byte\n",
      "Problematic folder path: /home/phucsaiyan/Documents/stage/testadddoc\n",
      "Error loading files with pattern '**/*.ipynb': [Errno 21] Is a directory: '/home/phucsaiyan/Documents/stage/testadddoc'\n",
      "Problematic folder path: /home/phucsaiyan/Documents/stage/testadddoc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from file_processing import load_and_index_files\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from jsonize_codebase import jsonize_code, generate_ctags_and_convert, generate_call_hierarchy\n",
    "from extract_info import extract_function_names_from_json, extract_function_detail_from_json\n",
    "from questions import ask_question, QuestionContext\n",
    "from utils import format_user_question\n",
    "from adddocstring import add_docstring_to_function\n",
    "from config import model_name, GREEN, RESET_COLOR, WHITE \n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "github_url = '/home/phucsaiyan/Documents/stage/testadddoc'\n",
    "repo_name = github_url.rsplit(\"/\", 1)[-1]\n",
    "output_path = github_url\n",
    "ctag_path,callgraph_path = jsonize_code(github_url, output_path)\n",
    "\n",
    "print(\"jsonize code is done\")\n",
    "functions = extract_function_names_from_json(ctag_path)\n",
    "index, documents, file_type_counts, filenames = load_and_index_files(github_url)\n",
    "if index is None:\n",
    "    print(\"No documents were found to index. Exiting.\")\n",
    "    exit()\n",
    "for function in functions:\n",
    "    function_dict = extract_function_detail_from_json(callgraph_path,function )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! api_key is not default parameter.\n",
      "                    api_key was transfered to model_kwargs.\n",
      "                    Please confirm that api_key is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_docstring_to_function\n",
      "generate a docstring for the function: add_docstring_to_function \n",
      "\u001b[32m\n",
      "ANSWER\n",
      "    \"\"\"\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function_name = \"add_docstring_to_function\"\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0.2)\n",
    "function_detail = function_dict[function_name]\n",
    "template = \"\"\"\n",
    "Repo: name: {function_name} | details: {function_detail} | Docs: {numbered_documents} | Q: {question} | FileCount: {file_type_counts} | FileNames: {filenames}\n",
    "Instr:\n",
    "Brief description of the function.\n",
    "    More detailed description of the function, including its purpose,\n",
    "    input parameters, and return value.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    parameter1 : data_type\n",
    "        Description of parameter1.\n",
    "    parameter2 : data_type\n",
    "        Description of parameter2.\n",
    "    Returns:\n",
    "    -------\n",
    "    return_type\n",
    "        Description of the return value.\n",
    "        \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[ \"function_name\", \"function_detail\", \"question\", \"numbered_documents\",\n",
    "                      \"file_type_counts\", \"filenames\"]\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "class QuestionContext:\n",
    "    def __init__(self, index, documents, llm_chain, model_name, function_name, function_detail, file_type_counts, filenames):\n",
    "        self.index = index\n",
    "        self.documents = documents\n",
    "        self.llm_chain = llm_chain\n",
    "        self.model_name = model_name\n",
    "        self.function_name = function_name\n",
    "        self.function_detail = function_detail\n",
    "        self.file_type_counts = file_type_counts\n",
    "        self.filenames = filenames\n",
    "question_context = QuestionContext(index, documents, llm_chain, model_name, function_name, function_detail, file_type_counts, filenames)\n",
    "user_question = f\"generate a docstring for the function: {function_name} \"\n",
    "user_question = format_user_question(user_question) \n",
    "\n",
    "question = user_question\n",
    "context.index = \n",
    "context.documents\n",
    "n_results=5\n",
    "query_tokens = clean_and_tokenize(query)\n",
    "bm25_scores = index.get_scores(query_tokens)\n",
    "# Compute TF-IDF scores\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=clean_and_tokenize, lowercase=True, stop_words='english', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([doc.page_content for doc in documents])\n",
    "query_tfidf = tfidf_vectorizer.transform([query])\n",
    "# Compute Cosine Similarity scores\n",
    "cosine_sim_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "# Combine BM25 and Cosine Similarity scores\n",
    "combined_scores = bm25_scores * 0.5 + cosine_sim_scores * 0.5\n",
    "# Get unique top documents\n",
    "unique_top_document_indices = list(set(combined_scores.argsort()[::-1]))[:n_results]\n",
    "\n",
    "\n",
    "\n",
    "user_question = f\"generate a docstring for the function: {function_name} \"\n",
    "#user_question = format_user_question(user_question)  \n",
    "answer = ask_question(user_question, question_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
